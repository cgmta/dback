# ----------------------------------------------------------------------------
# Default configuration for training a DPR embedding model on msmarco dataset.
# ----------------------------------------------------------------------------

# ------------------------------
# System Settings & Logging
# ------------------------------
backend: nccl                   # DDP backend: 'nccl' (GPU) or 'gloo' (CPU)
device: cuda                   # Training device
dtype: bfloat16                 # Data type: float32, float16, or bfloat16
complie: False                  # Use PyTorch 2.0 compile (for speed)
log_interval: 20                 # Log every N steps
wandb_log: False                 # Enable or disable logging
wandb_project: msmarco_ance  # WandB project name
wandb_run_name: embupd

# ------------------------------
# Training Parameters & Optimization Settings
# ------------------------------
epochs: 20            # Total training epochs
batch_size: 32        # Controls resource usage
n_docs: 500           # Number of documents per query
temperature: 8        # Important for training precision

# ------------------------------
# LR & Optimizer (AdamW)
# ------------------------------
decay_lr: True        # Enable learning rate decay
lr: 1e-7   # Maximum learning rate
min_lr: 5e-8           # Minimum learning rate
ctx_lr: 1e-6 # Learning rate for context encoder
max_iters: 60000      # Total training iterations
adam_epsilon: 1e-8    # Epsilon for Adam optimizer
weight_decay: 0.0
beta1: 0.9
beta2: 0.95
grad_clip: 1.0        # Clip gradients at this value (disable with 0.0)
warmup_steps: 20      # Number of warmup steps
lr_decay_iters: 60000 # Should be ~= max_iters per Chinchilla

# ------------------------------
# Evaluation & Index Updates
# ------------------------------
test: False            # Run test set evaluation if True
test_interval: 500   # Evaluate model after every N steps
update_ctx_emb: True  # Update embeddings dynamically
index_interval: 500  # Must be a multiple of test_interval
eval: True            # Evaluate model after every N steps
eval_interval: 40 # Avoid frequent evaluations if testing is enough
# large than max_match, save checkpoints to output dir
init_from: False      # Load model from this checkpoint if not None
max_match: 2000       # Top-k matches during training, used for saving checkpoints
best_acc_mrr_10: 0.25 # Best accuracy for MRR@10, used for early stopping
out_dir: ./train_output/ance_ms

# ------------------------------
# Dataset Paths
# ------------------------------
# Retrieve dataset
passage: ./MS-MARCO/marco_hf/msmarco-passage
passage_index: ./MS-MARCO/marco_hf/msmarco_ance_emb_index/compress_index/index
passage_embed: ./MS-MARCO/marco_hf/msmarco_ance_emb_index/reconstructed_embeddings.npy
# Training and Test dataset 
train_data: ./MS-MARCO/marco_hf/msmarco_train_dev
test_data: None

# ------------------------------
# Model & Tokenizer
# ------------------------------
n_embd: 768
model_type: ance
q_tokenizer: castorini/ance-msmarco-passage
q_name_or_path: castorini/ance-msmarco-passage

hydra:
  job:
    chdir: true   